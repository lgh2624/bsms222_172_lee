---
title: "Statistical inference"
output: html_notebook
---

# 15 Statistical inference
##
We first need to learn the basics of Statistical Inference, the part of statistics that helps distinguish patterns arising from signal from those arising from chance.
To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.

```{r}
library(tidyverse)
library(dslabs)
library(ggplot2)
```

## 15.1 Polls

The general goal of polls is to describe the opinions held by a specific population on a given set of topics. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. The statistic theory is referred to as inference and it is the main topic of this chapter.
Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources.
Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public.

In this section, we will show how the probability concepts we learned in the previous chapter can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values.

### 15.1.1 The sampling model for polls

To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize.
To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition.
The dslabs package includes a function that shows a random draw from this urn:
```{r}
take_poll(25)
```
Think about how you would construct your interval based on the data shown above.

The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors.

## 15.2 Populations, samples, parameters, and estimates

We want to predict the proportion of blue beads in the urn. Let’s call this quantity p,  1-p, and the spread 2p-1. The beads in the urn are called the population, the proportion of blue beads in the population p is called a parameter and the 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter p using the observed data in the sample.
An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample 0.48 must be at least related to the actual proportion p. But p is a random variable so we cannot say that p must be 0.48.
If we run the command `take_poll(25)` 4 times;
```{r}
replicate(4, take_poll(25))
```

### 15.2.1 The sample average

Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter p. Once we have this estimate, we can easily report an estimate for the spread 2p-1, but for simplicity we will illustrate the concepts for estimating p. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion p.
For random variable X of X=1 for blue one, than X=0 for red one. If we sample N beads, then the average of the draws $X_1, \cdots,X_N$ is equivalent to the proportion of blue beads in our sample. This is because adding the Xs is equivalent to counting the blue beads and dividing this count by the total N is equivalent to computing a proportion. We use the symbol $\bar{X}$ to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws divided by the constant N:
$$ \bar{X}=\frac{\displaystyle\sum_{i=1}^NX_i}{N} $$

For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. Even though we know that the expected value of the sum of draws is N times the average of the values in the urn and the average of the 0s and 1s in the urn must be p; the proportion of blue beads.
Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate p.

### 15.2.2 Parameters

Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters p to represent this quantity. p is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is p, we are going to estimate this parameter.
The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks.

### 15.2.3 Polling versus forecasting

Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the p for that moment and not for election day. The p for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.

### 15.2.4 Properties of our estimate: expected value and standard error

To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion $\bar{X}$.
Using what we have learned, the expected value of the sum $ N\bar{X} = Np$ So, $E(\bar{X}) = p$
We can also use what we learned to figure out the standard error: the standard error of the sum is $\sqrt{N}\sigma$, and for this example, the standard deviation of p is $(1-0)\sqrt{p{1-p)} = \sqrt{p(1-p)}$ Therefore, the standard error of p for N times is $ SE(\bar{X}) = \sqrt{\frac{p(1-p)}{N}} $.
The expected value of $\bar{X}$ is the parameter of interest p and we can make the standard error as small as we want by increasing N. The law of large numbers tells us that with a large enough poll, our estimate converges to p.
If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?
One problem is that we do not know p, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that p=0.51 and make a plot of the standard error versus the sample size N:
```{r}
p = 0.51
N = 10^seq(1, 5, 0.01)
SE = sqrt(p*(1-p)/sqrt(N))
qplot(log10(N), SE, geom = "line")
```

For N = 1000
```{r}
sqrt(p*(1-p))/sqrt(1000)
```

## 15.3 Exercises

1. Suppose you poll a population in which a proportion p of voters are Democrats and 1−p are Republicans. Your sample size is N=25. Consider the random variable S which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of p.
$$ E(S) = Np $$

2. What is the standard error of S? Hint: it’s a function of p.
$$ SE(S) = \sqrt{Np(1-p)} $$

3. Consider the random variable $\frac SN$. This is equivalent to the sample average, which we have been denoting as $\bar{X}$ . What is the expected value of the $\bar{X}$? Hint: it’s a function of p.
$$ E(\bar{X})=p $$

4. What is the standard error of $\bar{X}$? Hint: it’s a function of p.
$$ SE(\bar{X})=\sqrt{\frac{p(1-p)}{N}} $$

5. Write a line of code that gives you the standard error `se` for the problem above for several values of p, specifically for `p <- seq(0, 1, length = 100)`. Make a plot of `se` versus `p`.
```{r}
p <- seq(0, 1, length = 100)
se <- sqrt(p*(1-p)/25)
qplot(p, se, geom="line")
```

6. Copy the code above and put it inside a for-loop to make the plot for N=25, N=100, and N=1000.
```{r}
N <- c(25, 100, 1000)
p <- seq(0, 1, length = 100)
se <- lapply(N,
             function(N){mutate(as.data.frame(p),
                                se = sqrt(p*(1-p)/N),
                                N =N)})
se <- do.call(rbind, se)
se %>% ggplot() +
  geom_line(aes(p, se, group = N, color = N))
```

7. If we are interested in the difference in proportions, p−(1−p), our estimate is $d=\bar{X}−(1−\bar{X})$. Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of d.
$$E(d)=2p-1$$

8. What is the standard error of d?
$$ SE(d)=2SE(\bar{X}))=2\sqrt{\frac{p(1-p)}{N}}$$

9. If the actual p=0.45, it means the Republicans are winning by a relatively large margin since d=−0.1, which is a 10% margin of victory. In this case, what is the standard error of $2\bar{X}-1$ if we take a sample of N=25?
```{r}
p=0.45
N=25
2*sqrt(p*(1-p)/N)
```

10. Given the answer to 9, which of the following best describes your strategy of using a sample size of N=25? b

a. The expected value of our estimate $2\bar{X}-1$ is d, so our prediction will be right on.
b. Our standard error is larger than the difference, so the chances of $2\bar{X}-1$ being positive and throwing us off were not that small. We should pick a larger sample size.
c. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.
d. Because we don’t know p, we have no way of knowing that making N larger would actually improve our standard error.

## 15.4 Central Limit Theorem in practice

The CLT tells us that the distribution function for a sum of draws is approximately normal. We also learned that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of $\bar{X}$ is approximately normal.
In summary, we have that $\bar{X}$  has an approximately normal distribution with expected value p and standard error $\sqrt{\frac{p(1-p)}{N}}$
Now how does this help us? Suppose we want to know what is the probability that we are within 1% from p. We are basically asking what is $P(|\bar{X}-p|\leq 0.01)$ or, 
$$ P(\bar{X}\leq p+ 0.01) - P(\bar{X}\leq p-0.01)$$
Subtract the expected value and divide by the standard error to get a standard normal random variable, call it Z,
$$ P(Z\leq\frac{0.01}{SE(\bar{X})})-P(Z\leq-\frac{0.01}{SE(\bar{X})}) $$

One problem we have is that since we don’t know p, we don’t know $SE(\bar{X})$. But it turns out that the CLT still works if we estimate the standard error by using $SE(\bar{X})$ in place of p. We say that we plug-in the estimate. Our estimate of the standard error is therefore: $ \hat{SE}(\bar{X})=\sqrt{\frac{\bar{X}(1-\bar{X})}{N}}$ for this example, $\bar{X}=0.48$ and our estimate of standard error is:
```{r}
x_hat <- 0.48
se <- sqrt(x_hat*(1-x_hat)/25)
se
```

And now we can answer the question of the probability of being close to p. The answer is:
```{r}
pnorm(0.01/se) - pnorm(-0.01/se)
```

Therefore, there is a small chance that we will be close. A poll of only N=25 people is not really very useful, at least not for a close election.
Earlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:
```{r}
1.96*se
```

Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from p, we get:
$$ P(Z\le\frac{1.96SE(\bar{X})}{SE(\bar{X})})-P(Z\le-\frac{1.96SE(\bar{X})}{SE(\bar{X})}) $$
or,
$$ P(Z\leq1.96)-P(X\leq-1.96) $$
which we know is about 95%:
```{r}
pnorm(1.96)-pnorm(-1.96)
```

Hence, there is a 95% probability that $\bar{X}$ will be within $1.96\hat{SE}(\bar{X})$, in our case within about 0.2, of p. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.
In summary, the CLT tells us that our poll based on a sample size of 25 is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.
From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a $\bar{X}=0.48$ with a sample size of 2,000, our standard error $\hat{SE}(\bar{X})$ would have been 0.011. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.

### 15.4.1 A Monte Carlo simulation

Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:
```{r}
B <- 10000
N <- 1000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

The problem is, of course, we don’t know `p`. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function `take_poll(n=1000)` instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.

One thing we therefore do to corroborate theoretical results is to pick one or several values of `p` and run the simulations. Let’s set `p=0.45`. We can then simulate a poll:
```{r}
p <- 0.45
N <- 1000

x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
```

In this particular sample, our estimate is `x_hat`. We can use that code to do a Monte Carlo simulation:
```{r}
B <- 10000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

To review, the theory tells us that $\bar{X}$ is approximately normally distributed, has expected value p=0.45 and standard error $\sqrt{\frac{p(1-p)}{N}}=0.016$ The simulation confirms this:
```{r}
mean(x_hat)
sd(x_hat)
```

A histogram and qq-plot confirm that the normal approximation is accurate as well:
```{r}
# install.packages("gridExtra")
library(gridExtra)
grid.arrange(x_hat %>%
               as.data.frame() %>%
               ggplot() +
               geom_histogram(aes(x_hat), binwidth = 0.005, col = "black"),
             x_hat %>%
               as.data.frame() %>%
               ggplot() +
               geom_qq(aes(sample = x_hat)) +
               geom_qq_line(aes(sample = x_hat)),
             ncol=2)
```

Of course, in real life we would never be able to run such an experiment because we don’t know p. But we could run it for various values of p and N and see that the theory does indeed work well for most values.

### 15.4.2 The spread

The competition is to predict the spread, not the proportion p. However, because we are assuming there are only two parties, we know that the spread is 2p-1.  As a result, everything we have done can easily be adapted to an estimate of 2p-1. Once we have our estimate $\bar{X}$ and $\hat{SE}(\bar{X})$, we estimate the spread with $2\bar{X}-1$ and, since we are multiplying by 2, the standard error is $2\hat{SE}(\bar{X})$.
For our 25 item sample above, our estimate p is 0.48  with margin of error 0.20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for p, we have it for the spread 2p-1.

### 15.4.3 Bias: why not run a very large poll?

For realistic values of p, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:
```{r}
p = seq(0.35, 0.65, 0.001)
SE = 2*sqrt(p*(1-p)/100000)
qplot(p, SE, geom="line")
```

One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is p. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter.

## 15.5 Exercises

1. Write an urn model function that takes the proportion of Democrats p and the sample size N as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function `take_sample`.
```{r}
take_sample <- function(p, N){
  X <- replicate(N, sample(c(1, 0), 1, prob = c(p, 1-p)))
  mean(X)
}
```

2. Now assume `p <- 0.45` and that your sample size is N=100. Take a sample 10,000 times and save the vector of `mean(X) - p` into an object called `errors`. Hint: use the function you wrote for exercise 1 to write this in one line of code.
```{r}
p <- 0.45
N = 100
B = 10000
errors = replicate(B, take_sample(p, N)-p)
X = errors + p
```

3. The vector `errors` contains, for each simulated sample, the difference between the actual p and our estimate $\bar{X}$. We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions: c
```{r}
mean(errors)
hist(errors)
```
a. The errors are all about 0.05.
b. The errors are all about -0.05.
c. The errors are symmetrically distributed around 0.
d. The errors range from -1 to 1.

4. The error $\bar{X}-p$ is a random variable. In practice, the error is not observed because we do not know p. Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value $|\bar{X}-p|$?
```{r}
mean(abs(errors))
```

5. The standard error is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of `errors` rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?
```{r}
sd(errors)
```

6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of $\bar{X}$. What does theory tell us is the standard error of $\bar{X}$ for a sample size of 100?
$$ SE(\bar{X})=\sqrt{\frac{p(1-p)}{N}} $$
```{r}
sqrt(p*(1-p)/N)
```

7. In practice, we don’t know p, so we construct an estimate of the theoretical prediction based by plugging in $\bar{X}$ for p. Compute this estimate. Set the seed at 1 with `set.seed(1)`.
```{r}
set.seed(1)
x = replicate(B, take_sample(p, N))
sqrt(mean(x)*(1-mean(x))/N)
```

8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are.

> [1] 0.05024997 for Monte Carlo
> [1] 0.04974937 for theoretical prediction
> [1] 0.04974986 for estimate of theoretical prediction

The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict p with $\bar{X}$. Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for p=0.5. Create a plot of the largest standard error for N ranging from 100 to 5,000.
```{r}
p = 0.5
N <- seq(100, 5000, 1)
se = sqrt(p*(1-p)/N)
qplot(N, se) + geom_hline(aes(yintercept = 0.01))
```
Based on this plot, how large does the sample size have to be to have a standard error of about 1%? c

a. 100
b. 500
c. 2,500
d. 4,000

9. For sample size N=100, the central limit theorem tells us that the distribution of $\bar{X}$ is: b

a. practically equal to p.
b. approximately normal with expected value p and standard error $\sqrt{\frac{p(1-p)}{N}}$
c. approximately normal with expected value $\bar{X}$ and standard error $\sqrt{\frac{\bar{X}(1-\bar{X})}{N}}$
d. not a random variable.

10. Based on the answer from exercise 8, the error $\bar{X}-p$ is: b

practically equal to 0.
approximately normal with expected value 0 and standard error $\sqrt{\frac{p(1-p)}{N}}$.
approximately normal with expected value p and standard error $\sqrt{\frac{p(1-p)}{N}}$
not a random variable.

11. To corroborate your answer to exercise 9, make a qq-plot of the `errors` you generated in exercise 2 to see if they follow a normal distribution.
```{r}
errors %>% as.data.frame %>%
  ggplot(aes(sample = errors)) +
  geom_qq() + geom_qq_line()
```

12. If p=0.45 and N=100 as in exercise 2, use the CLT to estimate the probability that $\bar{X}\ge0.5$. You can assume you know p=0.45 for this calculation.
```{r}
p = 0.45
N = 100
se = sqrt(p*(1-p)/N)
1 - pnorm(p, se, 0.5)
```

13. Assume you are in a practical situation and you don’t know p. Take a sample of size N=100 and obtain a sample average of $\bar{X}=0.51$. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?
```{r}
p_hat = 0.51
N = 100
se = sqrt(p_hat*(1-p_hat)/N)
se
2*(1-pnorm(0.01/se))
```